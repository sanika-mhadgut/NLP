{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QnA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIirEU7Zp1IaU0LWpt6GFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanika-mhadgut/NLP/blob/master/QnA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPNfLSBrsS35",
        "colab_type": "text"
      },
      "source": [
        "#Name : Sanika Mhadgut\n",
        "#Branch : Btech Data Science Sem 6\n",
        "#Roll No: J031"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3pqld_SsdIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e77efa29-6e55-4f27-b533-73d1a7b744ad"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd \n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize \n",
        "from string import punctuation,digits\n",
        "nltk.download('stopwords')\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import models\n",
        "from gensim.test.utils import get_tmpfile, common_texts"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYIZjGnBuRgB",
        "colab_type": "text"
      },
      "source": [
        "Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LKooAsyteYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "112bfd12-b109-4a52-c973-527c6d06333d"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PlLHDjasSX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cd733ca6-945f-4b32-bbeb-85bef6f27f9e"
      },
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-22 16:18:22--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   140MB/s    in 0.3s    \n",
            "\n",
            "2020-02-22 16:18:27 (140 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNgLmzmRsUCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "df= pd.read_json('/content/train-v2.0.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzAUtZfBsYhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a26b9d94-e877-4c73-8817-cfa23b4ddf28"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Frédéric_Chopin', 'paragraphs': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sino-Tibetan_relations_during_the_M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'IPod', 'paragraphs': [{'qas': [{'qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'The_Legend_of_Zelda:_Twilight_Princ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Infection', 'paragraphs': [{'qas': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Hunting', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Kathmandu', 'paragraphs': [{'qas': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Myocardial_infarction', 'paragraphs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Matter', 'paragraphs': [{'qas': [{'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    version                                               data\n",
              "0      v2.0  {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1      v2.0  {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2      v2.0  {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3      v2.0  {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4      v2.0  {'title': 'The_Legend_of_Zelda:_Twilight_Princ...\n",
              "..      ...                                                ...\n",
              "437    v2.0  {'title': 'Infection', 'paragraphs': [{'qas': ...\n",
              "438    v2.0  {'title': 'Hunting', 'paragraphs': [{'qas': [{...\n",
              "439    v2.0  {'title': 'Kathmandu', 'paragraphs': [{'qas': ...\n",
              "440    v2.0  {'title': 'Myocardial_infarction', 'paragraphs...\n",
              "441    v2.0  {'title': 'Matter', 'paragraphs': [{'qas': [{'...\n",
              "\n",
              "[442 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la00sWlQsjr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtqEEGJJsY3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/train-v2.0.json\", \"r\") as f:\n",
        "    data = json.loads(f.read())\n",
        "\n",
        "questions, answers = [], []\n",
        "\n",
        "for i in range(len(data[\"data\"])):\n",
        "    for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
        "        for k in range(len(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])):\n",
        "            q = data[\"data\"][i][\"paragraphs\"][j][\"qas\"][k][\"question\"]\n",
        "            try: \n",
        "                a = data[\"data\"][i][\"paragraphs\"][j][\"qas\"][k][\"answers\"][0][\"text\"]\n",
        "            except IndexError:\n",
        "                a = \"None\"\n",
        "\n",
        "            questions.append(q)\n",
        "            answers.append(a)\n",
        "\n",
        "d = {\n",
        "    \"Questions\": questions,\n",
        "    \"Answers\": answers\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p656enxpsY58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data = pd.DataFrame(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqjjs-RysY72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "810870d2-a9db-405c-cd13-575f785a7c33"
      },
      "source": [
        "Data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130314</th>\n",
              "      <td>Physics has broadly agreed on the definition o...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130315</th>\n",
              "      <td>Who coined the term partonic matter?</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130316</th>\n",
              "      <td>What is another name for anti-matter?</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130317</th>\n",
              "      <td>Matter usually does not need to be used in con...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130318</th>\n",
              "      <td>What field of study has a variety of unusual c...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>130319 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Questions              Answers\n",
              "0                When did Beyonce start becoming popular?    in the late 1990s\n",
              "1       What areas did Beyonce compete in when she was...  singing and dancing\n",
              "2       When did Beyonce leave Destiny's Child and bec...                 2003\n",
              "3           In what city and state did Beyonce  grow up?        Houston, Texas\n",
              "4              In which decade did Beyonce become famous?           late 1990s\n",
              "...                                                   ...                  ...\n",
              "130314  Physics has broadly agreed on the definition o...                 None\n",
              "130315               Who coined the term partonic matter?                 None\n",
              "130316              What is another name for anti-matter?                 None\n",
              "130317  Matter usually does not need to be used in con...                 None\n",
              "130318  What field of study has a variety of unusual c...                 None\n",
              "\n",
              "[130319 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdankYcKsuTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "file = json.loads(open('/content/train-v2.0.json').read())\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "js = pd.io.json.json_normalize(file ,record_path)\n",
        "m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "m['context'] = idx\n",
        "js['q_idx'] = ndx\n",
        "main =pd.concat([m[['id','question','context','answers']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "main['c_id'] = main['context'].factorize()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLmZtSpxs5P2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e1e708b4-7855-44b9-8374-50155f6961bd"
      },
      "source": [
        "main.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[{'text': 'in the late 1990s', 'answer_start':...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[{'text': 'singing and dancing', 'answer_start...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[{'text': '2003', 'answer_start': 526}]</td>\n",
              "      <td>2003</td>\n",
              "      <td>526.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[{'text': 'Houston, Texas', 'answer_start': 166}]</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>166.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[{'text': 'late 1990s', 'answer_start': 276}]</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  56be85543aeaaa14008c9063  ...    0\n",
              "1  56be85543aeaaa14008c9065  ...    0\n",
              "2  56be85543aeaaa14008c9066  ...    0\n",
              "3  56bf6b0f3aeaaa14008c9601  ...    0\n",
              "4  56bf6b0f3aeaaa14008c9602  ...    0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk26IJi6s8hP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "6e664cc0-dc35-466c-8a80-685210cc6338"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "df1=TfidfVectorizer(stop_words='english',max_features=7000)\n",
        "df1_val=df1.fit_transform(main['question'])\n",
        "final=pd.DataFrame(df1_val.toarray(),columns=df1.get_feature_names())\n",
        "final.head(2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>101</th>\n",
              "      <th>10th</th>\n",
              "      <th>11</th>\n",
              "      <th>1100</th>\n",
              "      <th>11th</th>\n",
              "      <th>12</th>\n",
              "      <th>1200</th>\n",
              "      <th>122nd</th>\n",
              "      <th>12th</th>\n",
              "      <th>13</th>\n",
              "      <th>13th</th>\n",
              "      <th>14</th>\n",
              "      <th>1400</th>\n",
              "      <th>14th</th>\n",
              "      <th>15</th>\n",
              "      <th>1500</th>\n",
              "      <th>15th</th>\n",
              "      <th>16</th>\n",
              "      <th>1600</th>\n",
              "      <th>16th</th>\n",
              "      <th>17</th>\n",
              "      <th>1700</th>\n",
              "      <th>17th</th>\n",
              "      <th>18</th>\n",
              "      <th>1800</th>\n",
              "      <th>1800s</th>\n",
              "      <th>1810</th>\n",
              "      <th>1815</th>\n",
              "      <th>1820</th>\n",
              "      <th>1830</th>\n",
              "      <th>1837</th>\n",
              "      <th>1838</th>\n",
              "      <th>1839</th>\n",
              "      <th>1842</th>\n",
              "      <th>1848</th>\n",
              "      <th>1850</th>\n",
              "      <th>...</th>\n",
              "      <th>wwii</th>\n",
              "      <th>xbox</th>\n",
              "      <th>xi</th>\n",
              "      <th>xiang</th>\n",
              "      <th>xii</th>\n",
              "      <th>xxiii</th>\n",
              "      <th>yacht</th>\n",
              "      <th>yale</th>\n",
              "      <th>yangtze</th>\n",
              "      <th>yard</th>\n",
              "      <th>yards</th>\n",
              "      <th>yaroslav</th>\n",
              "      <th>year</th>\n",
              "      <th>yearly</th>\n",
              "      <th>years</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yeltsin</th>\n",
              "      <th>yiddish</th>\n",
              "      <th>yield</th>\n",
              "      <th>yoga</th>\n",
              "      <th>yongle</th>\n",
              "      <th>york</th>\n",
              "      <th>yorkers</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngest</th>\n",
              "      <th>youth</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yuan</th>\n",
              "      <th>yugoslavia</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zeppelin</th>\n",
              "      <th>zero</th>\n",
              "      <th>zhang</th>\n",
              "      <th>zhejiang</th>\n",
              "      <th>zinc</th>\n",
              "      <th>zone</th>\n",
              "      <th>zones</th>\n",
              "      <th>zoo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 7000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   000   10  100  1000  101  10th  ...  zhang  zhejiang  zinc  zone  zones  zoo\n",
              "0  0.0  0.0  0.0   0.0  0.0   0.0  ...    0.0       0.0   0.0   0.0    0.0  0.0\n",
              "1  0.0  0.0  0.0   0.0  0.0   0.0  ...    0.0       0.0   0.0   0.0    0.0  0.0\n",
              "\n",
              "[2 rows x 7000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NLBD8ETtHB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_digits = str.maketrans('', '', digits)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokens(str):\n",
        "  str = str.lower()\n",
        "  str = str.translate(remove_digits)\n",
        "  str = re.sub(r'[^\\w\\s]',' ',str)\n",
        "  token = word_tokenize(str)\n",
        "  words = [word for word in token if word not in stop_words and word not in punctuation]\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IMNtQ2BuoR_",
        "colab_type": "text"
      },
      "source": [
        "Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM8AQkSctPj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10283e36-80da-48aa-ffb9-ae659cc52cc1"
      },
      "source": [
        "df_final=main.question.apply(tokens)\n",
        "df_final.shape\n",
        "def clean(text):\n",
        "  text=text.lower()\n",
        "  remove_digits = str.maketrans('', '', digits)\n",
        "  text = text.translate(remove_digits)\n",
        "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "  tokenized_text=word_tokenize(text)\n",
        "  return tokenized_text\n",
        "df_final= main.question.apply(clean)\n",
        "df_final=df_final.tolist()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1V9QycEtidu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec=Word2Vec(list(df_final),size=100,min_count=1,sg=1,workers=5,window=3)\n",
        "s1='What is the question'\n",
        "s2='This is a Answer'\n",
        "sim=word2vec.wv.similarity(s1.lower().split(),s2.lower().split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnMIoRIIuaON",
        "colab_type": "text"
      },
      "source": [
        "size: The number of dimensions of the embeddings and the default is 100.\n",
        "\n",
        "\n",
        "window: The maximum distance between a target word and words around the target word. The default window is 3.\n",
        "\n",
        "\n",
        "min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 1.\n",
        "\n",
        "\n",
        "workers: The number of partitions during training and the default workers is 5.\n",
        "\n",
        "\n",
        "sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV9fpF8QtnKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}