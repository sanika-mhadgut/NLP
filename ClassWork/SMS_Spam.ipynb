{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Spam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCvtQwkL/ETdeRv5SINySV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanika-mhadgut/NLP/blob/master/SMS_Spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV0MeMlub6vm",
        "colab_type": "text"
      },
      "source": [
        "#Name : Sanika Mhadgut\n",
        "#Branch : Btech Data Science Sem 6\n",
        "#Roll No: J031\n",
        "\n",
        "Blacklisting is a technique that identifies IP addresses that send large amounts of spam. This method uses the number of recipients to determine if an email is spam or not. However, many legitimate emails can have high traffic volumes. Scanning message headings is a fairly reliable way to detect spam.\n",
        "\n",
        "\n",
        "\n",
        "Text mining (deriving information from text) is a wide field which has gained popularity with the huge text data being generated. Automation of a number of applications like sentiment analysis, document classification, topic classification, text summarization, machine translation, etc has been done using machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GSO0Q9Mb8UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJNicAPm4bIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zf = zipfile.ZipFile('smsspamcollection.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYHzovTI4rir",
        "colab_type": "code",
        "outputId": "dc7ed70f-c049-4c9f-8c55-2e285b0400af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "zf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<zipfile.ZipFile filename='smsspamcollection.zip' mode='r'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9fPxCs64jFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(zf.open('SMSSpamCollection'), sep='\\t',names=[\"label\", \"message\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAkejbsw4usr",
        "colab_type": "code",
        "outputId": "d77529c5-c223-4d44-e3a4-807fd2e83280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "data['message']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Go until jurong point, crazy.. Available only ...\n",
              "1                           Ok lar... Joking wif u oni...\n",
              "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3       U dun say so early hor... U c already then say...\n",
              "4       Nah I don't think he goes to usf, he lives aro...\n",
              "                              ...                        \n",
              "5567    This is the 2nd time we have tried 2 contact u...\n",
              "5568                 Will ü b going to esplanade fr home?\n",
              "5569    Pity, * was in mood for that. So...any other s...\n",
              "5570    The guy did some bitching but I acted like i'd...\n",
              "5571                           Rofl. Its true to its name\n",
              "Name: message, Length: 5572, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E8zoKjlcnAB",
        "colab_type": "text"
      },
      "source": [
        "a) Removal of stop words – Stop words like “and”, “the”, “of”, etc are very common in all English sentences and are not very meaningful in deciding spam or legitimate status, so these words have been removed from the emails.\n",
        "\n",
        "b) Lemmatization – It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider meaning of the sentence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez87n-N44v6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function that preprocesses the data for countvectoizer\n",
        "def Preprocessing(text):\n",
        "  #Initializing a dataframe of stopwords\n",
        "  stop_words = pd.read_csv(\"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\")\n",
        "  stop_words.values.tolist()\n",
        "\n",
        "  #Considering only alpha-numeric characters, numeric digits not even considered\n",
        "  text = re.findall(r'\\b[A-Za-z]\\w+\\b', text)\n",
        "  \n",
        "  #Remove stopwords\n",
        "  for word in text:\n",
        "    if word in stop_words:\n",
        "      text.remove(word)\n",
        "      \n",
        "  #Remove numerics\n",
        "  #res = ''.join([i for i in text if not i.isdigit()]) \n",
        "\n",
        "  #Remove @ and #\n",
        "  bad_chars = ['@', '#']\n",
        "  for i in text:\n",
        "    if i in bad_chars: \n",
        "      text = text.remove(i)\n",
        "  \n",
        "  text = list(text)\n",
        "\n",
        "  #Converting list to string\n",
        "  text = ' '.join(word for word in text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HSkyDwNcqwc",
        "colab_type": "text"
      },
      "source": [
        "Dictionary can be seen by the command print dictionary. You may find some absurd word counts to be high but don’t worry, it’s just a dictionary and you always have the scope of  improving it later. If you are following this blog with provided data-set, make sure your dictionary has some of the entries given below as most frequent words. Here I have chosen 3000 most frequently used words in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K827J6Ch6Lib",
        "colab_type": "code",
        "outputId": "52385015-a632-4fd4-efa7-2c4b2e539908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Preprocessing(data['message'][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Go until jurong point crazy Available only in bugis great world la buffet Cine there got amore wat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu7zHVRM6Wb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['preprocessed_text'] = data['message'].apply(Preprocessing,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUpthkKY6q52",
        "colab_type": "code",
        "outputId": "a39e32d7-f5b2-4619-cc5e-ed47ea4f9742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in wkly comp to win FA Cup final tk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>dun say so early hor already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah don think he goes to usf he lives around h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>This is the time we have tried contact have wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "      <td>Will going to esplanade fr home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>Pity was in mood for that So any other suggest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>The guy did some bitching but acted like be in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>Rofl Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label  ...                                  preprocessed_text\n",
              "0      ham  ...  Go until jurong point crazy Available only in ...\n",
              "1      ham  ...                              Ok lar Joking wif oni\n",
              "2     spam  ...  Free entry in wkly comp to win FA Cup final tk...\n",
              "3      ham  ...              dun say so early hor already then say\n",
              "4      ham  ...  Nah don think he goes to usf he lives around h...\n",
              "...    ...  ...                                                ...\n",
              "5567  spam  ...  This is the time we have tried contact have wo...\n",
              "5568   ham  ...                    Will going to esplanade fr home\n",
              "5569   ham  ...  Pity was in mood for that So any other suggest...\n",
              "5570   ham  ...  The guy did some bitching but acted like be in...\n",
              "5571   ham  ...                          Rofl Its true to its name\n",
              "\n",
              "[5572 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8yTqPZZ7glJ",
        "colab_type": "code",
        "outputId": "7a25ad5e-9262-4749-adef-482a6f083c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "corpus = data['preprocessed_text'] \n",
        "corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Go until jurong point crazy Available only in ...\n",
              "1                                   Ok lar Joking wif oni\n",
              "2       Free entry in wkly comp to win FA Cup final tk...\n",
              "3                   dun say so early hor already then say\n",
              "4       Nah don think he goes to usf he lives around h...\n",
              "                              ...                        \n",
              "5567    This is the time we have tried contact have wo...\n",
              "5568                      Will going to esplanade fr home\n",
              "5569    Pity was in mood for that So any other suggest...\n",
              "5570    The guy did some bitching but acted like be in...\n",
              "5571                            Rofl Its true to its name\n",
              "Name: preprocessed_text, Length: 5572, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-MMAHI7lvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFK2sGR_8fYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorize= TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zoymgeU8l2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fitting the model and passing our sentences right away:\n",
        "response= vectorize.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QFGVJSL8ueZ",
        "colab_type": "code",
        "outputId": "4ded367c-0f91-4ae4-c6fa-540ec0b3a2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "print(response)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 7473)\t0.18241264829651851\n",
            "  (0, 231)\t0.32647198856800297\n",
            "  (0, 2764)\t0.15305130991688437\n",
            "  (0, 6843)\t0.1555161950550194\n",
            "  (0, 1210)\t0.27580485521143805\n",
            "  (0, 914)\t0.3116528020516887\n",
            "  (0, 3650)\t0.27580485521143805\n",
            "  (0, 7697)\t0.22083291550052703\n",
            "  (0, 2804)\t0.18034330636364296\n",
            "  (0, 916)\t0.27580485521143805\n",
            "  (0, 3263)\t0.1069931616636402\n",
            "  (0, 4720)\t0.15602976712614566\n",
            "  (0, 465)\t0.24419040033995174\n",
            "  (0, 1487)\t0.25283008183768235\n",
            "  (0, 5107)\t0.25535167546045223\n",
            "  (0, 3523)\t0.32647198856800297\n",
            "  (0, 7233)\t0.23001810878216972\n",
            "  (0, 2720)\t0.14787418026870422\n",
            "  (1, 4716)\t0.5466243141314314\n",
            "  (1, 7599)\t0.43162957585464123\n",
            "  (1, 3491)\t0.5236804332035243\n",
            "  (1, 3686)\t0.4083258549263009\n",
            "  (1, 4687)\t0.2718944069420321\n",
            "  (2, 318)\t0.1849800442797567\n",
            "  (2, 5433)\t0.1849800442797567\n",
            "  :\t:\n",
            "  (5570, 951)\t0.2830195559272965\n",
            "  (5570, 2638)\t0.2753741356210262\n",
            "  (5570, 2054)\t0.24409532149526603\n",
            "  (5570, 6248)\t0.20541570643501375\n",
            "  (5570, 943)\t0.13633148775441348\n",
            "  (5570, 7269)\t0.2088816708041\n",
            "  (5570, 1755)\t0.18433209615766674\n",
            "  (5570, 4516)\t0.2101107882421641\n",
            "  (5570, 600)\t0.14290139247632946\n",
            "  (5570, 6823)\t0.10220399800152388\n",
            "  (5570, 2472)\t0.12145828902705748\n",
            "  (5570, 6238)\t0.18433209615766674\n",
            "  (5570, 3791)\t0.159545141701127\n",
            "  (5570, 246)\t0.11224405842241922\n",
            "  (5570, 7520)\t0.18730237914237433\n",
            "  (5570, 3394)\t0.12207280777305733\n",
            "  (5570, 2954)\t0.1706162309351053\n",
            "  (5570, 6955)\t0.08360476870460612\n",
            "  (5570, 2522)\t0.159545141701127\n",
            "  (5570, 3263)\t0.11153278693235086\n",
            "  (5571, 5697)\t0.5576716097580104\n",
            "  (5571, 7087)\t0.42359036289726915\n",
            "  (5571, 3402)\t0.5785362120911208\n",
            "  (5571, 4425)\t0.39090923235845254\n",
            "  (5571, 6955)\t0.14852408386067228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1txe5Ubg8vfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweetsVect'] = list(vectoriser.fit_transform(df['tweets']).toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
